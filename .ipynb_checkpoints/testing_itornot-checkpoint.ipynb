{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.layers import Dropout\n",
    "from tensorflow import keras\n",
    "from keras import preprocessing\n",
    "from keras import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions for preprocessing and turning into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#creating sentences\n",
    "def create_sents(text):\n",
    "    nltk.download('punkt')\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    return(sentences)\n",
    "\n",
    "#padding\n",
    "def padding_data(sentences, index, maxlen=25):\n",
    "    new_sentences = []\n",
    "    for sentence in sentences:\n",
    "        #this will give us a sentence converted to numerical array\n",
    "        sentence = text_to_word_sequence(sentence) \n",
    "        new_sentence = []\n",
    "        words = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                word = index[word]\n",
    "            except:\n",
    "                KeyError\n",
    "                #for unknown words we encounter:\n",
    "                word = 0 \n",
    "            words.append(word)\n",
    "        new_sentence.append(words)\n",
    "        new_sentence = preprocessing.sequence.pad_sequences(new_sentence, maxlen=maxlen, padding='post')\n",
    "        new_sentences.append(new_sentence[0])\n",
    "    return(new_sentences)\n",
    "\n",
    "#downloading stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#returing word index\n",
    "def get_index(filename): #for opening that json file\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return(data)\n",
    "word_index = get_index('word_index.json')\n",
    "\n",
    "#opening and processing a file\n",
    "def get_data(filename):\n",
    "    with open(filename, 'r', encoding='unicode_escape') as f:\n",
    "        data = f.read()\n",
    "    data = data.lower()\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = stopwords.words('english')\n",
    "    data = data.split()\n",
    "    data = [w for w in data if w not in stop_words]\n",
    "    data = \" \".join(data)\n",
    "\n",
    "    return(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3qLoFkWvdLI"
   },
   "source": [
    "# Test model on a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ZZOA-KQUvORJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f93de6e4040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "[[0.02006012]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#uploading the model\n",
    "model = keras.models.load_model('models/it_or_not.model')\n",
    "\n",
    "#function for preparing a sentence or a text\n",
    "def predict_string(string):\n",
    "    prep_sent = [w.lower() for w in string.split() if w not in stop_words]\n",
    "    prep_sent = ' '.join(prep_sent)\n",
    "    test_sents = create_sents(prep_sent)\n",
    "    test_padded = padding_data(test_sents, word_index, maxlen=50)\n",
    "    test_array = np.array(test_padded)\n",
    "    \n",
    "    prediction = model.predict(test_array)\n",
    "    print(prediction)\n",
    "\n",
    "t_sent1 = 'Peppa Pig has received her excellency in her palace'\n",
    "predict_string(t_sent1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9926444]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "t_sent2 = 'Machine learning (ML) is the study of computer algorithms that can improve automatically through experience and by the use of data.'\n",
    "predict_string(t_sent2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['machine learning (ml) study computer algorithms improve automatically experience use data.', '[1] seen part artificial intelligence.', 'machine learning algorithms build model based sample data, known training data, order make predictions decisions without explicitly programmed so.', '[2] machine learning algorithms used wide variety applications, medicine, email filtering, speech recognition, computer vision, difficult unfeasible develop conventional algorithms perform needed tasks.', '[3] subset machine learning closely related computational statistics, focuses making predictions using computers; machine learning statistical learning.', 'study mathematical optimization delivers methods, theory application domains field machine learning.', 'data mining related field study, focusing exploratory data analysis unsupervised learning.', '[5][6] implementations machine learning use data neural networks way mimics working biological brain.', '[7][8] application across business problems, machine learning also referred predictive analytics.']\n",
      "[0.99908704]\n",
      "[0.70825285]\n",
      "[0.9996601]\n",
      "[0.99991703]\n",
      "[0.999739]\n",
      "[0.9882073]\n",
      "[0.99198604]\n",
      "[0.99537146]\n",
      "[0.99576795]\n",
      "AVERAGE: 0.964221; the text is IT-oriented\n"
     ]
    }
   ],
   "source": [
    "def predict_file(filename):\n",
    "    text = get_data(filename)\n",
    "    test_sents = create_sents(text)\n",
    "    print(test_sents)\n",
    "    test_padded = padding_data(test_sents, word_index, maxlen=25)\n",
    "    test_array = np.array(test_padded)\n",
    "    \n",
    "    prediction = model.predict(test_array)\n",
    "    \n",
    "    number = 0\n",
    "    \n",
    "    for pred in prediction:\n",
    "            print(pred)\n",
    "            pred = float(pred)\n",
    "            number = number + pred\n",
    "\n",
    "    average = round((number / len(prediction)), 7)\n",
    "    str_average = str(average)\n",
    "    if average >= 0.5:\n",
    "        print('AVERAGE:', str_average+';', 'the text is IT-oriented')\n",
    "    else:\n",
    "        print('AVERAGE:', str_average+';', 'the text is ordinary (not IT-oriented')\n",
    "\n",
    "prediction = predict_file('data/testing.TXT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_sent3 = input('Enter a sentence: ')\n",
    "predict_sent(t_sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "testing_itornot",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
